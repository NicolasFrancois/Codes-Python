scikit-learn

MODULES A IMPORTER
# Import des bibliothèques nécessaires
from sklearn import linear_model
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# Lecture du fichier "admissions.csv"
df = pd.read_csv('admissions.csv')

# Affichage des premières lignes
df.head()

#voir les infos d'un df
df.info()

#Supprimer les valeurs nulles d'un df
df = df.dropna()

## Créer des classes de performances
test_gre = pd.cut(x = df['gre'],
                  bins = [200, 450, 550, 620, 800],
                  labels = ['mauvais', 'moyen', 'moyen +', 'bon'])
                  
                  
## les croiser pour afficher la distribution
pd.crosstab(df['admit'], test_gre, normalise=column) on met normalise si on veut un % en colonne (column) ou en ligne (line)

## Dichotomisation des classes crées en 0/1
df = df.join(pd.get_dummies(df.niveaux_notes, prefix='niveau')) #on dichotomise la variable niveaux_notes (mauvais, moyen, moyen+...) en y rajouter le préfixe 'niveau' (niveau_mauvais : 1 ou 0 ...) pour nommer les colonnes et on les joint au DF crée
df = df.join(pd.get_dummies(df.test_gre, prefix='gre'))

# Création d'un df avec les données qui vont nous servir pour le marchine learning:
data = df.iloc[:,4:13]
target = df['admit'] #on définit la cible de notre prédiction

# Décomposition aléatoire des données en deux ensembles d'entraînement et de test
# par défaut l'échantillon est aléatoirement réparti
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=66) # avec comme hypothèse que 20% des données seront pour le test random_state = 66 pour une reproductivité de l'aléatoire

----------------------------------------------------------------------------------------------------------
Classification par régression logistique
----------------------------------------------------------------------------------------------------------
# Création du classifieur et construction du modèle sur les données d'entraînement
clf = linear_model.LogisticRegression(C=1.0) #creation
clf.fit(X_train, y_train)#entrâinement
df.head()

##Prédire les données de l'ensemble de test et stocker les dans la variable y_pred
y_pred = clf.predict(X_test)

# Calcul de la matrice de confusion 
## Méthode 1 : à l'aide de sklearn
from sklearn.metrics import confusion_matrix 
cm = confusion_matrix(y_test,y_pred)
print(cm)

## Méthode 2 : à l'aide de pandas
cm = pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])
cm

## Calculer le taux de bonnes prédictions du modèle.
clf.score(X_test, y_test)

## Afficher le rapport de classification de nos prédictions.
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

##Prédiction selon modèle et comparation avec le réel
probs = clf.predict_proba(X_test) #Créer un tableau probs contenant les probabilités pour les individus de X_test d'appartenir à la classe 0 ou la classe 1.
y_preds = np.where(probs[:,1]>0.4,1,0) #Créer un vecteur y_preds qui, pour chaque ligne de probs vaut 1 si la probabilité d'appartenir à la classe 1 est supérieure à 0.4, et 0 sinon.
cm = pd.crosstab(y_test, y_preds, rownames=['Classe réelle'], colnames=['Classe prédite']) #Afficher une matrice de confusion entre les vrais labels de y_test et y_preds
cm #voir la diagonale pour voir la si la distribution réel est en phase avec les prédictions

## Création Courbe ROC
from sklearn.metrics import roc_curve, auc #Importer les fonctions roc_curve() et auc().

fpr, tpr, seuils = roc_curve(y_test, probs[:,1], pos_label=1) #Appliquer la fonction roc_curve() à y_test et la deuxième colonne de probs, en précisant que le label positif dans notre cas est 1. Stocker les résultats retournés dans les tableaux fpr, tpr, seuils.
roc_auc = auc(fpr, tpr) #Calculer dans roc_auc l'AUC correspondant aux valeurs de fpr et tpr

import matplotlib.pyplot as plt

plt.plot(fpr, tpr, color='orange', lw=2, label='Modèle clf (auc = %0.2f)' % roc_auc) #avec affichage des spécificités
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Aléatoire (auc = 0.5)') #avec affichage des spécificités
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Taux faux positifs')
plt.ylabel('Taux vrais positifs')
plt.title('Courbe ROC')
plt.legend(loc="lower right")
plt.show();

--------------------------------------------------------------------------------
Support Vector Machine (SVM) que des variables numériques

from sklearn import svm
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn importpreprocessing
import pandas as pd
import numpy as np

#Diviser les matrices définies ci-dessus en un ensemble d'entraînement et de test. Précisément, data sera divisé en X_train et X_test et target sera divisé en y_train et y_test.
X_train, X_test, y_train, y_test= train_test_split(data, target, test_size=0.2)

#Créer un tableau X_train_scaled, contenant les données centrées réduites de X_train // centrer réduire
X_train_scaled = preprocessing.scale(X_train)
print(X_train_scaled.mean(axis=0)) #Afficher la moyenne de chaque colonne de X_train_scaled.
print(X_train_scaled.std(axis=0)) Afficher l'écart type de chaque colonne de X_train_scaled.

#Il faut passer pas une étape de fit afin de ne pas changer la performance du modèle après intégration de données réelle, cette étape de garder les données d'entrainement pour le modèle
#Créer un objet scaler comme ci-dessus et l'appliquer à X_train pour retourner le tableau X_train_scaled centré-réduit.
scaler = preprocessing.StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)

#Puis il faut prépaere les données de test dans le même moulin
X_test_scaled = scaler.transform(X_test)
print(X_test_scaled.mean(axis=0))
print(X_test_scaled.std(axis=0))

--------------------------------------------------------------------------------
Classification par machines à vecteurs de support 


#Créer un classifieur clf, ayant pour paramètres gamma=0.01 et kernel='poly', en utilisant la méthode SVC du package svm.
clf = svm.SVC(gamma=0.01,  kernel='poly')

#Entraîner l'algorithme sur l'ensemble d'entraînement (X_train_scaled et y_train du modèlede SVM).
clf.fit(X_train_scaled, y_train)

--------------------------------------------------------------------------------
Évaluation du modèle de classification

# Effectuer les prédictions sur l'ensemble de test et les stocker dans la variable y_pred.
y_pred = clf.predict(X_test_scaled)

#Afficher une matrice de confusion à partir de ces prédictions
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

#Configurer des hyperparamètres
#Créer un dictionnaire parametres contenant les valeurs possibles prises pour le paramètre C:[0.1,1,10], pour kernel: ['rbf', 'linear','poly'] et pour gamma:[0.001, 0.1, 0.5].
parametres = {'C':[0.1,1,10], 'kernel':['rbf','linear', 'poly'], 'gamma':[0.001, 0.1, 0.5]}

#Appliquer la fonction model_selection.GridSearchCV() au modèle clf, en lui spécifiant dans l'argument param_grid la grille de paramètres créée plus haut. Retourner le classifieur ainsi créé dans grid_clf.
L'argument scoring permet de choisir la métrique que l'on souhaite utiliser pour évaluer la performance des modèles, par défaut c'est l'accuracy.
grid_clf = model_selection.GridSearchCV(estimator=clf, param_grid=parametres)

#Entraîner grid_clf sur l'ensemble d'entraînement, (X_train_scaled, y_train). Sauvegarder les résultats dans l'objet grille.
grille = grid_clf.fit(X_train_scaled,y_train)

#Afficher les meilleurs paramètres de la grille pour notre modèle grid_clf.
print(grid_clf.best_params_) #best_params_ est  un module crée par DataScientest pour l'exercice

#Effectuer à présent les prédictions de classes grâce au modèle grid_clf sur l'ensemble de test et les stocker dans la variable y_pred
y_pred = grid_clf.predict(X_test_scaled)

#Afficher une matrice de confusion à partir de ces prédictions
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

--------------------------------------------------------------------------------
Méthode des k plus proches voisins (K nearest neighbors (KNN)) reconnu pour la reconnaissance de formes, les algorithmes de compression, le marketing ciblé

# Import des bibliothèques nécessaires
from sklearn import neighbors
from sklearn import datasets
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

#Importer le dictionnaire des données dans digits grâce à la fonction datasets.load_digits().
digits = datasets.load_digits()

#Sauvegarder dans un DataFrame X_digits les données contenues dans l'attribut data de digits.
X_digits = pd.DataFrame(digits.data)
#Sauvegarder dans y_digits les labels contenus dans l'attribut target de digits.
y_digits = digits.target


##Diviser les matrices X_digits et y_digits en un ensemble d'apprentissage de 80% (X_train, y_train) et un ensemble de test (X_test, y_test) de 20%.
Rajouter l'argument random_state = 126 dans la fonction train_test_split pour la reproductibilité du choix de l'aléatoire.
X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, test_size=0.2, random_state=126)


## 2. Apprentissage des données 

#Créer un classifieur knn, avec k = 7 et la distance 'minkowski' (distance par défaut, avec p=2, distance euclidienne).
knn = neighbors.KNeighborsClassifier(n_neighbors=7, metric='minkowski')

# Ajuster le classifieur sur l'ensemble d'entraînement (X_train et y_train).
knn.fit(X_train, y_train)

## 3. Évaluation du modèle de classification 
#Appliquer le modèle aux données de l'ensemble de test et stocker les prédictions obtenues dans la variable y_pred.
y_pred = knn.predict(X_test)

#Afficher une matrice de confusion pour comparer les classes réelles et prédites.
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

## Faire autre modèle pour voir quel est le plus performant
#Créer un nouveau classifieur knn_m, avec k=5 et la distance 'manhattan'
knn_m = neighbors.KNeighborsClassifier(n_neighbors=5, metric='manhattan')
#Ajuster le nouveau classifieur sur l'ensemble d'entraînement (X_train et y_train).
knn_m.fit(X_train, y_train)

#Calculer le score de performance ('accuracy') pour les deux modèles.
#Score du modèle utilisant la distance de Minkowski
score_minkowski = knn.score(X_test, y_test)

#Score du modèle utilisant la distance de Manhattan
score_manhattan = knn_m.score(X_test, y_test)

score_minkowski, score_manhattan

##Si score proche alors il faut en faire la représentation graphique
#Créer trois listes score_minko, score_man, score_cheb, dans lesquelles vous stockerez les scores de 3 modèles utilisant respectivement les matrices de Minkowski, Manhattan et Chebyshev, pour des valeurs de k allant de 1 à 40.
score_minko = []
score_man = []
score_cheb = []

for k in range(1, 41):
    knn = neighbors.KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    score_minko.append(knn.score(X_test, y_test))

for k in range(1, 41):
    knn = neighbors.KNeighborsClassifier(n_neighbors=k, metric='manhattan')
    knn.fit(X_train, y_train)
    score_man.append(knn.score(X_test, y_test))
    
for k in range(1, 41):
    knn = neighbors.KNeighborsClassifier(n_neighbors=k, metric='chebyshev')
    knn.fit(X_train, y_train)
    score_cheb.append(knn.score(X_test, y_test))
    
#Afficher dans un graphique les listes créées en fonction de la valeur de k.
# et Utiliser des couleurs et légendes différentes pour différencier les métriques.
import matplotlib.pyplot as plt
%matplotlib inline

plt.plot(range(1, 41), score_minko, color='blue', linestyle='dashed', lw=2, label='Minkowski')
plt.plot(range(1, 41), score_man, color='orange', linestyle='dashed', lw=2, label='Manhattan')
plt.plot(range(1, 41), score_cheb, color='red', linestyle='dashed', lw=2, label='Chebyshev')
plt.title('Score - valeur de K')  
plt.xlabel('Valeur de K')  
plt.ylabel('Accuracy') 
plt.legend(); #Vérifier si la variation des modèles est importante et la stabilité sur X, voir quand le modèle décroit pour savoir si quel X appliquer la bonne méthode

--------------------------------------------------------------------------------
Arbres de décision fonctionnent aussi bien avec des variables discrètes ou qualitatives (après dichotomisation sur scikit-learn) gérer à la fois des variables numériques et catégorielles, et s'avèrent facilement interprétables.

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import pandas as pd

Créer le Data Frame data contenant les différentes features et le vecteur target contenant la variable cible 'diagnosis'.
data = bc_data.drop('diagnosis', axis=1) #s'il n'y a qu'une seule variable à exclure du df de Machine learning
target = bc_data.diagnosis

#2. Apprentissage des données

#Séparer le jeu de données en ensembles d'apprentissage et de test, de sorte que l'ensemble de test représente 20% des données totales. Rajouter l'argument random_state=123 dans la fonction train_test_split pour la reproductibilité du choix de l'aléatoire.
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=123)

# Créer une instance DecisionTreeClassifier appelée dt_clf, avec le critère criterion='entropy' et l'argument max_depth=4 pour spécifier le nombre maximum de points de séparation possible avant d'atteindre un nœud "feuille". À nouveau, ajouter l'argument random_state=123 pour la reproductibilité des résultats.
dt_clf = DecisionTreeClassifier(criterion ='entropy', max_depth=4, random_state=123)

#Entraîner le classifieur sur l'ensemble d'entraînement.
dt_clf.fit(X_train, y_train)

#Appliquer le modèle aux données de l'ensemble de test et stocker les prédictions obtenues dans la variable y_pred.
y_pred = dt_clf.predict(X_test)

#Afficher une matrice de confusion pour comparer les classes réelles et prédites.
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

#Afficher les 8 variables les plus importantes pour dt_clf, ainsi que leurs importances respectives
feats = {}
for feature, importance in zip(data.columns, dt_clf.feature_importances_):
    feats[feature] = importance 
    
importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})
importances.sort_values(by='Importance', ascending=False).head(8)

##Deux mesures d'impureté peuvent être utilisées pour les arbres de décisions dans scikit-learn : l'Information Gain ou Entropy et l'index de Gini (plus de détails sur ces mesures ici).

#Créer un classifieur dt_clf_gini, ayant pour paramètres : criterion='gini' , max_depth=4 et random_state=321.
dt_clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=321)

#Entraîner le nouveau modèle sur l'ensemble d'entraînement (X_train et y_train).
dt_clf_gini.fit(X_train, y_train)

#Sauvegarder les prédictions du modèle sur X_test dans y_pred.
y_pred = dt_clf_gini.predict(X_test)

#Afficher la matrice de confusion correspondante.
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

#Afficher, comme pour le modèle précédent, les huit variables les plus importantes, ainsi que leurs importances respectives.
feats = {}
for feature, importance in zip(data.columns, dt_clf_gini.feature_importances_):
    feats[feature] = importance 

importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})

#Affichage des 8 variables les plus importantes
importances.sort_values(by='Gini-importance', ascending=False).head(8)


--------------------------------------------------------------------------------
Le Boosting

# Import des bibliothèques nécessaires
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import pandas as pd
import numpy as np

#Charger le CSV
df = pd.read_csv('letter-recognition.csv')
df.info()

#Séparer les features dans un DataFrame nommé data, et la variable cible dans target
data = df.drop('letter', axis=1)
target = df['letter']

#Séparer les datasets en en un jeu d'apprentissage et un jeu de test. La taille du jeu de test doit correspondre à 30 % de la quantité totale des données disponibles.
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3)

## Classification supervisée par arbre de décision
# Construction du modèle de classification en utilisant les données d'entraînement
dtc = DecisionTreeClassifier(max_depth=5) #Créer un modèle de classification dtc, ayant pour paramètre max_depth=5, en utilisant la méthode DecisionTreeClassifier.
dtc.fit(X_train, y_train) #Entraîner l'algorithme sur l'ensemble d'entraînement (X_train et y_train).

## Évaluation du premier modèle de classification
#Calculer le taux de bonnes prédictions du modèle de classification sur l'ensemble de test en utilisant la méthode score
dtc.score(X_test, y_test)

#Effectuer les prédictions du modèle sur X_test, et afficher la matrice de confusion correspondante.
y_pred = dtc.predict(X_test)
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

## Algorithme de Boosting

# Création du classifier et construction du modèle sur les données d'entraînement
ac = AdaBoostClassifier(base_estimator=dtc, n_estimators=400) Créer de nouveau un classifieur ac, ayant pour paramètres : base_estimator=dtc, et n_estimators=400, en utilisant la classe AdaBoostClassifier du package ensemble.
ac.fit(X_train, y_train) #Entraîner l'algorithme sur l'ensemble (X_train, y_train).

# Calcul de l'accuracy
ac.score(X_test, y_test) #Calculer l'accuracy (taux de bonnes prédiction) du nouveau modèle de classification sur l'échantillon de test.

#  Afficher la matrice de confusion de l'ensemble de test.
y_pred = ac.predict(X_test)
pd.crosstab(y_test, y_pred)


--------------------------------------------------------------------------------
Le Bagging

# Création du classifier et construction du modèle sur les données d'entraînement
from sklearn.ensemble import BaggingClassifier #Importer BaggingClassifier depuis sklearn.ensemble.
bc = BaggingClassifier(n_estimators=1000, oob_score=True) #Créer un nouveau classifieur bc, ayant pour paramètres : n_estimators=1000, et oob_score=True pour calculer l'erreur Out Of Bag.
bc.fit(X_train, y_train) #Entraîner l'algorithme sur l'ensemble d'entraînement (X_train et y_train). 
bc.oob_score_ #Afficher l'erreur Out Of Bag du modèle grâce à l'attribut oob_score_.
bc.oob_score_

#Calculer l'accuracy (taux de bonnes prédiction) du nouveau modèle de classification sur l'échantillon de test.
bc.score(X_test, y_test)

#Afficher la matrice de confusion de l'ensemble de test.
y_pred = bc.predict(X_test)
pd.crosstab(y_test, y_pred)

--------------------------------------------------------------------------------
Les forêts aléatoires

# Import des bibliothèques nécessaires
from sklearn import ensemble
from sklearn.model_selection import train_test_split
import pandas as pd


# Lecture du fichier 'churn_dataset.csv'
churn_df = pd.read_csv('churn_dataset.csv')

#Séparer dans target, la colonne churn de churn_df pour isoler la variable que nous voulond prédire
target = churn_df['churn']

#Transformer la variable international plan en variables indicatrices et les ajouter aux colonnes de churn_df et Transformer la variable voice mail plan en variables indicatrices et les ajouter aux colonnes de churn_df..car c'est une variable qaulitative (de oui/non à1/0)
churn_df = churn_df.join(pd.get_dummies(churn_df['international plan'], prefix='international'))
churn_df = churn_df.join(pd.get_dummies(churn_df['voice mail plan'], prefix='voicemail'))

## Créer le DataFrame data à partir de chun_df après avoir supprimé les variables 'international plan', 'voice mail plan', 'state','area code', 'phone number' et 'churn' qui va servir de base pour prédiction
to_drop = ['international plan', 'voice mail plan', 'state', 'area code', 'phone number', 'churn'] ## Suppression de variables
data = churn_df.drop(to_drop,axis=1)

## Séparer le jeu de données en ensembles d'apprentissage et de test, de sorte que l'ensemble de test représente 20% des données totales et Rajouter l'argument random_state=12 dans la fonction train_test_split pour la reproductibilité du choix de l'aléatoire.
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=12)

## Apprentissage des données

#En utilisant la méthode [RandomForestClassifier] du module ensemble, créer un Classificateur clf ayant pour paramètre : n_jobs=-1. Rajouter l'argument random_state=321, pour la reproductibilité du choix de l'aléatoire.
clf = ensemble.RandomForestClassifier(n_jobs=-1, random_state=321)

# Entraîner l'algorithme sur l'ensemble d'entraînement (X_train et y_train).
clf.fit(X_train, y_train)

#Prédire les données de l'ensemble de test et les stocker dans la variable y_pred.
y_pred = clf.predict(X_test)

# Afficher la matrice de confusion de l'échantillon test.
pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite'])

#Calculer le taux de bonnes prédictions de la classification à l'aide de la fonction score.
clf.score(X_test, y_test)

#Calculer les probabilités pour X_test d'appartenir à chacune des deux classes. Stocker ensuite ces probabilités dans la variable y_probas. On utilisera la méthode predict_proba()).
y_probas = clf.predict_proba(X_test)

#Importer matplotlib.pyplot sous le diminutif plt.
#Importer scikitplot sous le diminutif skplt.
# Afficher la courbe de gain cumulée avec y_test et y_probas.

import matplotlib.pyplot as plt
import scikitplot as skplt

skplt.metrics.plot_cumulative_gain(y_test, y_probas, figsize=(12,8))
plt.show()

--------------------------------------------------------------------------------
Jeux de données déséquilibrés

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler,  ClusterCentroids
from imblearn.metrics import classification_report_imbalanced, geometric_mean_score
from sklearn.metrics import f1_score
from sklearn.svm import SVC

# Remplacer males et female en 1 ou 0
df['Gender'] = df['Gender'].replace({'Male': 0, 'Female': 1}

# Séparer les données en un DataFrame feats contenant les variables explicatives et target contenant la variable Exited.
feats = df.drop('Exited', axis=1)
target = df['Exited']

#Transformer la variable NumOfProducts en plusieurs variables indicatrices à l'aide de la fonction get_dummies de pandas.
df = df.join(pd.get_dummies(df.NumOfProducts, prefix='_'))

#Créer un ensemble d'entraînement et un ensemble de test correspondant respectivement à 75% et 25% des données.
X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.25)

#Appliquer une standardisation des variables continues ('CreditScore', 'Age', 'Balance', 'EstimatedSalary') calculée sur le jeu d'entraînement et appliquée aux jeux d'entraînement et de test.
cols = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']

sc = StandardScaler()
X_train[cols] = sc.fit_transform(X_train[cols])
X_test[cols] = sc.transform(X_test[cols])

#Appliquer une standardisation des variables continues ('CreditScore', 'Age', 'Balance', 'EstimatedSalary') calculée sur le jeu d'entraînement et appliquée aux jeux d'entraînement et de test.
cols = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']

sc = StandardScaler()
X_train[cols] = sc.fit_transform(X_train[cols])
X_test[cols] = sc.transform(X_test[cols])

#Afficher la distribution en pourcentage de la variable cible.
target.value_counts(normalize=True)

## 1er modèle : SVM simple
# Créer un modèle de classification SVM, en ajoutant l'argument gamma='scale'.
svm = SVC(gamma='scale')
#Entraîner le modèle sur l'ensemble d'entraînement, puis afficher le score obtenu sur l'ensemble de test.
svm.fit(X_train, y_train)

#Effectuer les prédictions de notre modèle sur les données de test, et afficher la matrice de confusion correspondante.
y_pred = svm.predict(X_test)
print(pd.crosstab(y_test, y_pred, colnames=['Predictions']))

#Afficher le rapport d'évaluation du modèle sur l'échantillon de test.
print(classification_report_imbalanced(y_test, y_pred))

--------------------------------------------------------------------------------
L'Oversampling

# A partir de X_train et y_train, créer deux nouveaux ensembles de données (X_ro, y_ro) et (X_sm, y_sm) obtenus respectivement par Oversampling aléatoire (RandomOverSampler) et SMOTE (SMOTE) et Afficher le nombre d'éléments de chaque classe du nouvel ensemble obtenu.
rOs = RandomOverSampler()
X_ro, y_ro = rOs.fit_resample(X_train, y_train)
print('Classes échantillon oversampled :', dict(pd.Series(y_ro).value_counts()))

#SMOTE
smo = SMOTE()
X_sm, y_sm = smo.fit_resample(X_train, y_train)
print('Classes échantillon SMOTE :', dict(pd.Series(y_sm).value_counts()))

#Créer un modèle de classification SVM, en ajoutant l'argument gamma='scale'.
svm = SVC(gamma='scale')
svm.fit(X_ro, y_ro)

#Entraîner le modèle sur l'ensemble issu du RandomOversampler, puis afficher la matrice de confusion obtenue grâce aux prédictions sur l'ensemble de test.
y_pred = svm.predict(X_test)
print(pd.crosstab(y_test, y_pred))

#Afficher le rapport de résultat sur l'ensemble de test obtenu par classification_report_imbalanced.
print(classification_report_imbalanced(y_test, y_pred))

#Créer un modèle de classification SVM, en ajoutant l'argument gamma='scale'.
svm = SVC(gamma='scale')
svm.fit(X_sm, y_sm)

#Entraîner le modèle sur l'ensemble issu du SMOTE, puis afficher la matrice de confusion obtenue grâce aux prédictions sur l'ensemble de test.
y_pred = svm.predict(X_test)

# Afficher le rapport de résultat sur l'ensemble de test obtenu par classification_report_imbalanced.
print(pd.crosstab(y_test, y_pred))
print(classification_report_imbalanced(y_test, y_pred))


--------------------------------------------------------------------------------
L'Undersampling

#A partir de X_train et y_train, créer deux nouveaux ensembles de données (X_ru, y_ru) et (X_cc, y_cc) obtenus respectivement par Undersampling aléatoire (RandomUnderSampler) et l'application de ClusterCentroidset Afficher le nombre d'éléments de chaque classe du nouvel ensemble obtenu.
#Random Undersampling
rUs = RandomUnderSampler()
X_ru, y_ru = rUs.fit_resample(X_train, y_train)
print('Classes échantillon undersampled :', dict(pd.Series(y_ru).value_counts()))

#Centroids
cc = ClusterCentroids()
X_cc, y_cc = cc.fit_resample(X_train, y_train)

#Créer un modèle de classification SVM, en ajoutant l'argument gamma='scale'.
svm = SVC(gamma='scale')
svm.fit(X_ru, y_ru)

#Entraîner le modèle sur l'ensemble issu du RandomUnderSampler, puis afficher la matrice de confusion obtenue grâce aux prédictions sur l'ensemble de test.
y_pred = svm.predict(X_test)

#Afficher le rapport de résultat sur l'ensemble de test obtenu par classification_report_imbalanced.
print(pd.crosstab(y_test, y_pred))
print(classification_report_imbalanced(y_test, y_pred))

#Créer un modèle de classification SVM, en ajoutant l'argument gamma='scale'.
svm = SVC(gamma='scale')

#Entraîner le modèle sur l'ensemble issu du ClusterCentroids, puis afficher la matrice de confusion obtenue grâce aux prédictions sur l'ensemble de test.
svm.fit(X_cc, y_cc)
y_pred = svm.predict(X_test)
print(pd.crosstab(y_test, y_pred))

#Afficher le rapport de résultat sur l'ensemble de test obtenu par classification_report_imbalanced.
print(classification_report_imbalanced(y_test, y_pred))


--------------------------------------------------------------------------------
Autres méthodes


